{"meta":{"title":"개발새발 블로그","subtitle":"개발새발 블로그","description":null,"author":"Hyunkyung Ahn","url":"https://tuwhit.github.io"},"pages":[],"posts":[{"title":"Principles of Effective Story Writing - The Pivotal Labs Way","slug":"userstory","date":"2018-08-01T11:51:29.000Z","updated":"2018-08-02T13:11:49.016Z","comments":true,"path":"2018/08/01/userstory/","link":"","permalink":"https://tuwhit.github.io/2018/08/01/userstory/","excerpt":"","text":"pivotal labs에서 pivotal tracker를 이용하여 효과적으로 스토리를 작성하는 방법에 대한 아티클을 대충 번역해서 정리해봄. 스토리란?대화와 그 문맥에 대한 placeholder작은 디테일들을 포함할 필요가 없으나, 이야기한 내용을 상기시켜야 하며 모든것을 high level에서 포함해야함how 가 아닌 what 이 진술되어야 함기술적으로 규범화된것이 아니므로, 엔지니어링 팀이 문제의 기능을 구현할 수있는 정확한 정보를 지정할 필요가 없음기능의 이유와 그 기능이 무엇인지 정확하게 설명해야함모든 스토리는 INVEST 모델을 따라야함 INVEST modelIndependent : 의존되지않고 릴리즈 할수있어야함Negotiable : 토론할 준비가 돼있고 팀 인풋에 따라 조정될수있음Valuable : end user 에게 가치를 제공함Estimatable : 개발팀은 스토리의 복잡성을 추정할 수 있음Small : 실제 가치를 제공하는 선에서 가능한 작아야함testable : 테스트의 승인 기준을 포함 다양한 스토리 타입들기능 / 사용자 스토리 (feature)기능 스토리는 유저에게 가치를 제공하는 제품에 추가될수 있는 가장 작은 추가적 기능의 누구에게, 무엇을, 왜를 설명할수 있도록 디자인됨기능 스토리는 개발팀에 의해 포인트가 매겨지고 기능을 완료하는데 걸리는 시간이 아닌 복잡도로 평가됨유저의 관점에서 작성되고 개발팀의 가벼운 요구사항 문서로서의 역할을 함INVEST 모델에 따라 이들은 독립적이어야하며 사용자에게 확실한 가치를 제공해야함 feature에 포함되어야 하는 것들 title 제목은 짧아야하며 설명가능하고 특정 사용자나 개인을 포함해야함 예를들어 사용자/개인은 단순히 ‘사용자’가 아닌 특정 타입의 사용자거나 개인이름(예: 토마스)이어야 함 사람이 아니라 시스템이 사용자인 경우에도 동일하게 작용함(예: 구매 API) business case 누가, 왜, 무엇을 원하는지를 서술 팀의 모든 구성원들이 해당 기능이 추가되어야하는 이유를 이해할 수 있어야함 이유를 생각할수 없다면 그 기능이 포함되어야 하는지 다시 판단해봐야함 비지니스 케이스를 통해 팀원들은 제공된것보다 나은 사용자 경험이 있는지 생각해볼수 있음 acceptance criteria (수락기준?) 스토리가 완료되었는지 확인하기 위해 따라야할 사항을 정의함 해당 스토리를 작업(?)한 개발자는 그것을 제공하기전에 수락 기준을 따라야함 syntax GIVEN [필요한 context] WHEN [action] THEN [reaction] 수용 기준에 여러번의 ‘and’를 쓰는걸 발견한다면 스토리를 더 쪼개야 함 notes 스토리에 필요한 추가적인 정보를 포함함 ex: 디자인노트, 개발자 노트… resources 기능스토리를 전달하는데 도움을 주는 것들 ex: 목업, 와이어프레임, 유저플로우, 링크 등 labels 스토리들을 그룹핑하는데 효과적임 ex: Epics, 빌드 넘버, 사용자… bug버그는 이미 수락된 feature의 결함버그를 사용해서 새 기능을 자세히 설명하면 안됨(ex: 가격은 음수가 아니어야함, 로그인 버튼이 작동하지않음)버그들은 이미 전달된 기능과 직접적으로 연관돼있으며 새로운 유저 가치를 제공하지 않으므로 포인트가 없음. 왜냐하면 추정하기가 불가능하고 해결하는데 30초가 걸릴수도 30일이 걸릴수도 있기때문 bug에 포함되어야 하는 것들 title : 짧고 설명가능해야함 description : 현재 무슨일이 일어나고 있는지, 무슨 일이 일어나야 하는가를 서술 instructions : 버그를 재현하는 단계를 요약하셈 resources : 스크린샷이나 버그를 설명하는데 도움을 주는 것들 chorechore는 필요하지만 사용자에게 직접적이고 분명한 가치를 제공하지는 않음 (ex: 테스트환경을 위해 새 도메인 및 와일드카드 SSL 인증서를 설치, 시스템 문제해결을 위한 툴 평가)chore는 유저 가치에 직접적으로 기여하지 않으므로 측정할수 없음chore가 유저가치를 제공하는것처럼 느껴지면 featrue stroy에 통합되어야함예를들어 분석서비스를 사용하는 경우 서비스 설치에 관한 추가적인 복잡성은 chore로 분리되지 말고 feature story에 고려되어야함 chore 스토리에 포함되어야 하는 것들 title : 짧고 설명가능해야함 description : 왜 필요한지, 이것이 팀을 더 빠르게하거나 처리되지않으면 코드베이스에서 문제를 일으킬수 있는 의존성이 있는가를 서술 resources : 작업 수행을 돕는 지침, 추가 컨텍스트등","categories":[],"tags":[{"name":"pivotal tracker","slug":"pivotal-tracker","permalink":"https://tuwhit.github.io/tags/pivotal-tracker/"},{"name":"pivotal labs","slug":"pivotal-labs","permalink":"https://tuwhit.github.io/tags/pivotal-labs/"},{"name":"유저 중심 스토리","slug":"유저-중심-스토리","permalink":"https://tuwhit.github.io/tags/유저-중심-스토리/"}]},{"title":"Facebook Webhooks API로 lead ads와 CRM 통합","slug":"facebook-webhook","date":"2018-07-30T12:14:40.000Z","updated":"2018-08-02T13:12:44.349Z","comments":true,"path":"2018/07/30/facebook-webhook/","link":"","permalink":"https://tuwhit.github.io/2018/07/30/facebook-webhook/","excerpt":"","text":"이 페이지는 이 문서를 따라 테스트해보면서 추가적인 내용을 적어둔것이니 문서를 먼저 읽는것이 좋음! 미리 설정해두면 좋은것들 App 설정 Page 설정 및 Lead Ad 설정 App, Page 관리자 권한 얻기 테스트를 하면서 알았는데 페이스북 페이지에서 바로 webhook 설정을 할수 있는게 아니었다;Lead ad는 페이스북 페이지에서, webhook은 페이스북 앱에서 각각 설정해줘야하고, 페이스북 페이지를 앱이 구독할수 있도록 승인이 필요하다.즉 Lead ad의 실시간 업데이트가 앱으로 전달되고, 앱에 설정된 webhook을 통해 Lead ad로 받은 고객정보를 처리할수 있게된다. 문서를 보면 웹서버에 php 파일을 생성하라고 나와있는데, 대신 AWS Lambda와 API Gateway를 사용해서 테스트했다. 로컬서버로 테스트해보려다 잘안돼서 빠른 포기 -.-;;API Gateway에서 메소드 생성시 GET, POST 둘다 생성해줘야한다. (아마 페이지 구독 설정시에는 GET, 실제 lead ad payload 전송시엔 POST 로 보내는듯) 각 메소드의 Integration Request에서 Use lambda proxy integration에 체크하면 Lambda 함수에 대한 입력을 요청 헤더, 경로 변수, 쿼리 문자열 파라미터 및 본문의 조합으로 표현할 수 있다.해당 API로 들어오는 리퀘스트의 내용을 그대로 Lambda 함수로 전달하고 이것을 event로 쉽게 핸들링할수 있다. 12345678910111213141516# 구독설정시 lambda 함수 예제def lambda_handler(event, context): challenge = event['queryStringParameters']['hub.challenge'] verify_token = event['queryStringParameters']['hub.verify_token'] # if verify_token == 'test_token': # print(challenge) response = &#123; \"isBase64Encoded\": False, \"statusCode\": 200, \"headers\": &#123;&#125;, \"body\": challenge &#125; return response 대신 response 형식도 아래와 같이 맞춰줘야한다.123456&#123; \"isBase64Encoded\": true|false, \"statusCode\": httpStatusCode, \"headers\": &#123; \"headerName\": \"headerValue\", ... &#125;, \"body\": \"...\"&#125; 모든 설정을 완료한 후에 https://developers.facebook.com/tools/lead-ads-testing 에서 테스트를 해보면 제대로 동작해보는지 확인할수 있다.앱 관리 페이지에서 webhook - leadgen Test 에서도 테스트 가능하다. body에 유저가 입력한 form data가 같이 넘어오는줄 알았는데 그게 아니었음… 실시간 업데이트에 대한 데이터가 아래처럼 넘어온다. ㅠㅠ123456789101112131415161718192021222324252627282930313233array( \"object\" =&gt; \"page\", \"entry\" =&gt; array( \"0\" =&gt; array( \"id\" =&gt; 153125381133, \"time\" =&gt; 1438292065, \"changes\" =&gt; array( \"0\" =&gt; array( \"field\" =&gt; \"leadgen\", \"value\" =&gt; array( \"leadgen_id\" =&gt; 123123123123, \"page_id\" =&gt; 123123123, \"form_id\" =&gt; 12312312312, \"adgroup_id\" =&gt; 12312312312, \"ad_id\" =&gt; 12312312312, \"created_time\" =&gt; 1440120384 ) ), \"1\" =&gt; array( \"field\" =&gt; \"leadgen\", \"value\" =&gt; array( \"leadgen_id\" =&gt; 123123123124, \"page_id\" =&gt; 123123123, \"form_id\" =&gt; 12312312312, \"adgroup_id\" =&gt; 12312312312, \"ad_id\" =&gt; 12312312312, \"created_time\" =&gt; 1440120384 ) ) ) ) )) 그래서 넘어온 id를 가지고 Facebook Ads API를 이용해서 데이터를 받아와야한다 ㅠㅠ 페이스북놈들…123456789101112131415161718# Python Business SDKfrom facebookads.adobjects.lead import Leadfrom facebookads.api import FacebookAdsApiaccess_token = '&lt;ACCESS_TOKEN&gt;'app_secret = '&lt;APP_SECRET&gt;'app_id = '&lt;APP_ID&gt;'id = '&lt;ID&gt;'FacebookAdsApi.init(access_token=access_token)fields = []params = &#123;&#125;print Lead(id).get( fields=fields, params=params,) 1234567891011121314151617181920212223242526272829303132# response&#123; \"created_time\": \"2015-02-28T08:49:14+0000\", \"id\": \"&lt;LEAD_ID&gt;\", \"ad_id\": \"&lt;AD_ID&gt;\", \"form_id\": \"&lt;FORM_ID&gt;\", \"field_data\": [&#123; \"name\": \"car_make\", \"values\": [ \"Honda\" ] &#125;, &#123; \"name\": \"full_name\", \"values\": [ \"Joe Example\" ] &#125;, &#123; \"name\": \"email\", \"values\": [ \"joe@example.com\" ] &#125;, &#123; \"name\": \"selected_dealer\", \"values\": [ \"99213450\" ] &#125;], ...&#125;","categories":[],"tags":[{"name":"Lambda","slug":"Lambda","permalink":"https://tuwhit.github.io/tags/Lambda/"},{"name":"AWS","slug":"AWS","permalink":"https://tuwhit.github.io/tags/AWS/"},{"name":"Facebook","slug":"Facebook","permalink":"https://tuwhit.github.io/tags/Facebook/"},{"name":"lead ads","slug":"lead-ads","permalink":"https://tuwhit.github.io/tags/lead-ads/"},{"name":"API Gateway","slug":"API-Gateway","permalink":"https://tuwhit.github.io/tags/API-Gateway/"}]},{"title":"DynamoDB Stream","slug":"dynamostream","date":"2018-06-24T11:55:31.000Z","updated":"2018-06-24T12:42:36.527Z","comments":true,"path":"2018/06/24/dynamostream/","link":"","permalink":"https://tuwhit.github.io/2018/06/24/dynamostream/","excerpt":"","text":"DynamoDB 스트림은 DynamoDB 테이블 항목의 변경 정보에 대한 흐름을 나타내준다.스트림 이벤트는 INSERT, MODIFY, DELETE 세가지로 나눠진다. DDB 테이블의 overview 탭에 보면 Stream Detail 항목이 있는데 여기서 뷰 타입을 설정 가능하다.이벤트가 발생한 아이템의 키만 보여주던지, 새 이미지만 보여주던지, 기존 이미지만 보여주던지, 둘다 보여주던지 총 네가지 타입이 있다.물론 스트림을 비활성화하는것도 가능하다. Trigger 탭에서 스트림에 대한 Lambda 트리거를 쉽게 추가할 수 있다.새 Lambda function을 추가할수도 있고, 존재하는 function을 추가할 수도 있다.function을 추가하면서 batch size도 설정 가능한데 나중에 설정해둔 사이즈를 확인할 수 있는 곳을 못찾아서 한참 헤맸다;해당 Lambda function에 들어가서 Designer 항목에서 DynamoDB를 선택하면 아래에 정보를 띄워준다.이걸 몇달 후에나 알았음 -.- 수백만건의 데이터를 DynamoDB에 때려넣다가 에러는 안나는데 결과 데이터의 오차율이 너무 커서 꽤 오래 고생을 했는데 원인은 단순했다.DynamoDB Stream은 최대 24시간동안만 저장한다는 사실…트리거 function이 단순한 로직이 아니라면 스트림이 쌓여있다가 트리거가 돌기전에 휘발될수 있으니 주의하자 ㅠㅠ","categories":[],"tags":[{"name":"DynamoDB","slug":"DynamoDB","permalink":"https://tuwhit.github.io/tags/DynamoDB/"},{"name":"AWS","slug":"AWS","permalink":"https://tuwhit.github.io/tags/AWS/"},{"name":"Stream","slug":"Stream","permalink":"https://tuwhit.github.io/tags/Stream/"}]},{"title":"Lambda, Kinesis Firehose 를 이용해서 AuroraDB에 추가된 데이터를 실시간으로 ES에 저장하기","slug":"aurora-to-es","date":"2018-02-15T04:41:08.000Z","updated":"2018-02-15T05:07:49.362Z","comments":true,"path":"2018/02/15/aurora-to-es/","link":"","permalink":"https://tuwhit.github.io/2018/02/15/aurora-to-es/","excerpt":"","text":"이번엔 Kinesis Firehose 의 목적지를 Elasticsearch로 설정해서 테스트해봤다. Elasticsearch 새 도메인 올리는데 시간이 좀 걸리니 미리 만들어 두는게 좋음. Lambda, Kinesis Firehose 를 이용해서 AuroraDB에 추가된 데이터 실시간으로 캡쳐하기 문서 참고해서 AuroraDB, Lambda 설정까지 완료 새 Elasticsearch domain을 생성 kinesis firehose가 elasticsearch 6.0은 지원을 안하니 그 아래 버전으로 생성할것. Elasticsearch 6.0 is not currently supported by Kinesis Firehose. Contact AWS Support for more information. 에러가 뜬다… 따흐흑… 위 domain에 index, type을 생성 123456789101112131415&#123; \"properties\": &#123; \"ItemID\": &#123;\"type\": \"integer\"&#125;, \"Category\": &#123;\"type\": \"text\"&#125;, \"Price\": &#123;\"type\": \"float\"&#125;, \"Quantity\": &#123;\"type\": \"integer\"&#125;, \"OrderDate\": &#123; \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" &#125;, \"DestinationState\": &#123;\"type\": \"text\" &#125;, \"ShippingType\": &#123;\"type\": \"text\"&#125;, \"Referral\": &#123;\"type\": \"text\"&#125; &#125;&#125; 새 Kinesis Firehose 를 생성 destination을 Amazon Elasticsearch service 선택 Amazon Elasticsearch Service destination에 위에서 만든 elasticsearch domain, index, type을 입력 IAM에 ES에 대한 권한이 제대로 명시돼 있는지 확인 (알아서 만들어줌) Lambda code 수정 123456789101112131415161718192021222324import boto3import jsonimport loggingfirehose = boto3.client('firehose', region_name='리전이름')stream_name = 'delivery stream 이름'def lambda_handler(event, context): # for ES firehose_data = &#123; \"ItemID\": event['ItemID'], \"Category\": event['Category'], \"Price\": event['Price'], \"Quantity\": event['Quantity'], \"OrderDate\": event['OrderDate'].split()[0] + \"T\" + event['OrderDate'].split()[1], \"DestinationState\": event['DestinationState'], \"ShippingType\": event['ShippingType'], \"Referral\": event['Referral'] &#125; firehose_data = &#123;'Data': json.dumps(firehose_data)&#125; logging.info(json.dumps(firehose_data)) result = firehose.put_record(DeliveryStreamName=stream_name,Record=firehose_data) AuroraDB 테이블에 데이터 추가 얼마후에 ES에 _search 쿼리 날려보면 데이터가 추가된것을 확인할수 있음 기타 참고 kinesis 에서 데이터가 잘 넘어가는지 CloudWatch에서 확인실패시 S3에 로그저장됨 Datetime 포맷은 ES의 Date 포맷에 맞춰서 넣을것","categories":[],"tags":[{"name":"Lambda","slug":"Lambda","permalink":"https://tuwhit.github.io/tags/Lambda/"},{"name":"AWS","slug":"AWS","permalink":"https://tuwhit.github.io/tags/AWS/"},{"name":"Kinesis Firehose","slug":"Kinesis-Firehose","permalink":"https://tuwhit.github.io/tags/Kinesis-Firehose/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://tuwhit.github.io/tags/Elasticsearch/"},{"name":"AuroraDB","slug":"AuroraDB","permalink":"https://tuwhit.github.io/tags/AuroraDB/"}]},{"title":"Lambda, Kinesis Firehose 를 이용해서 AuroraDB에 추가된 데이터 실시간으로 캡쳐하기","slug":"aurora-data-streaming","date":"2018-01-23T11:23:50.000Z","updated":"2018-01-23T11:44:29.997Z","comments":true,"path":"2018/01/23/aurora-data-streaming/","link":"","permalink":"https://tuwhit.github.io/2018/01/23/aurora-data-streaming/","excerpt":"","text":"데이터 스트리밍 관련 리서치를 하다가 이런 문서가 있길래 직접 해봤다.(AuroraStream이 따로 없고 DynamoStream만 있는듯) 회사에선 AuroraDB를 써서 일단 요 실습을 따라해보는걸로. 사실 자세한건 위 링크에 다 나와있다…ㅎㅎ;; 간단히 요약하면서 중간중간 삽질한것만 추가로 기록해봤음. Kinesis 에서새로운 delivery stream을 생성하고 Destination을 S3로 선택해준뒤, 원하는 버켓을 지정해줌 AuroraDB 와 같은 리전에서 새 Lambda function 을 생성 Lambda 코드에서 stream_name 만 1번에서 만든 stream 이름으로 변경 혹시 firehose 가 다른 리전에 있다면 firehose = boto3.client(‘firehose’, region_name=’리전 이름’) 이렇게 뒤에 리전 이름 추가해줌 AuroraDB에서 원하는 테이블에 프로시저, 트리거 생성 프로시저에서 2에서 생성해준 Lambda function의 arn 으로 설정해줌 AuroraDB parameter group에서 aws_default_lambda_role 의 value 를 lambda 를 실행시킬수있는 role의 arn으로 설정 안해주면 Missing Credentials: Cannot instrantiate Lambda Client 에러남 테이블에 데이터를 추가 Lambda에서 예외처리를 따로 안해줘서 필드가 null 일땐 에러나는듯 좀있다가 S3 버킷 확인해보면 요렇게 파일이 생겨있음 그래도 에러나면 IAM 제대로 설정돼있는지 확인 ㄱㄱ","categories":[],"tags":[{"name":"Lambda","slug":"Lambda","permalink":"https://tuwhit.github.io/tags/Lambda/"},{"name":"S3","slug":"S3","permalink":"https://tuwhit.github.io/tags/S3/"},{"name":"AWS","slug":"AWS","permalink":"https://tuwhit.github.io/tags/AWS/"},{"name":"Kinesis Firehose","slug":"Kinesis-Firehose","permalink":"https://tuwhit.github.io/tags/Kinesis-Firehose/"},{"name":"AuroraDB","slug":"AuroraDB","permalink":"https://tuwhit.github.io/tags/AuroraDB/"}]},{"title":"Data Pipeline로 S3에서 DynamoDB로 데이터 import 시키기","slug":"import-to-dynamodb","date":"2017-11-19T08:57:45.000Z","updated":"2017-11-19T09:45:30.000Z","comments":true,"path":"2017/11/19/import-to-dynamodb/","link":"","permalink":"https://tuwhit.github.io/2017/11/19/import-to-dynamodb/","excerpt":"","text":"이번엔 Data Pipeline을 이용해서 S3에서 DynamoDB로 데이터를 import 시키는 방법을 정리해본다. DynamoDB에 item을 생성하는데(이틀간 삽질하며) 아래의 방식들을 써봤다. AWS console에서 직접 생성 ruby SDK를 이용해서 csv 파일을 한줄한줄 읽어서 item 생성 aws dynamodb batch-write-item –request-items file://파일명 명령어를 이용(형식 맞춰줘야함) DataPipeline을 이용해서 S3의 파일을 import 1억건의 데이터를 넣는데는 1, 2번으론 무리가 있었고 3번도 파일 용량제한이 있는지 에러를 내뱉었다. 결국 Data Pipeline으로… Data Pipeline 을 생성하면 DynamoDB 템플릿이 존재한다. Import DynamoDB backup data from S3 을 선택하고 생성해준다. 그리고 Parameter 들만 잘 설정해주면 됨. 유의해야할것은 Data Pipeline을 생성한 리전과 S3 bucket, DynamoDB의 리전이 같아야한다. 안그러면 에러를 뿜뿜함. 아니 리전 입력하라고 해놓고 왜 에러나는지 모를… 파일형식도 맞춰야한다. 열심히 구글링을 해봐도 없어서 새로 Data Pipeline을 생성해서 반대로 Export 시켜봤다 ㅠㅠ 123&#123;\"amount\":&#123;\"s\":\"8\"&#125;,\"shop_id\":&#123;\"s\":\"100000\"&#125;,\"gender\":&#123;\"s\":\"1\"&#125;,\"Id\":&#123;\"n\":\"1\"&#125;,\"user_id\":&#123;\"s\":\"70167727\"&#125;,\"date\":&#123;\"s\":\"2017-01-22\"&#125;,\"birth_year\":&#123;\"s\":\"1955\"&#125;&#125;&#123;\"amount\":&#123;\"s\":\"17\"&#125;,\"shop_id\":&#123;\"s\":\"100001\"&#125;,\"gender\":&#123;\"s\":\"0\"&#125;,\"Id\":&#123;\"n\":\"2\"&#125;,\"user_id\":&#123;\"s\":\"29015489\"&#125;,\"date\":&#123;\"s\":\"2017-07-05\"&#125;,\"birth_year\":&#123;\"s\":\"2010\"&#125;&#125;&#123;\"amount\":&#123;\"s\":\"11\"&#125;,\"shop_id\":&#123;\"s\":\"100001\"&#125;,\"gender\":&#123;\"s\":\"1\"&#125;,\"Id\":&#123;\"n\":\"4\"&#125;,\"user_id\":&#123;\"s\":\"90567446\"&#125;,\"date\":&#123;\"s\":\"2017-01-14\"&#125;,\"birth_year\":&#123;\"s\":\"1953\"&#125;&#125; 그 결과, item별로 {필드1: {데이터 형식: 값}, 필드2: {데이터 형식: 값}} 요런 형태로 파일이 만들어졌다. 그래서 기존의 csv 파일을 위 형태로 컨버팅후(스크립트 만들어서 돌림ㅠㅠ) Data Pipeline을 실행시켜보니 잘 들어감! 그런데 write 속도가 너무 느려서 DynamoDB의 write capacity 값을 올려줬는데도 일정값 이상으로 안올라오길래 왜그런가 했더니 Data Pipeline 설정에 myDDBWriteThroughputRatio 필드가 있었다.(0 ~ 1 사이의 값으로 입력가능) 1으로 수정해주니 입력값으로 잘 올라옴. 다른건 몰라도 이제 DynamoDB에 데이터 입력하는덴 전문가된듯. -_-","categories":[],"tags":[{"name":"DynamoDB","slug":"DynamoDB","permalink":"https://tuwhit.github.io/tags/DynamoDB/"},{"name":"Data Pipeline","slug":"Data-Pipeline","permalink":"https://tuwhit.github.io/tags/Data-Pipeline/"},{"name":"S3","slug":"S3","permalink":"https://tuwhit.github.io/tags/S3/"},{"name":"AWS","slug":"AWS","permalink":"https://tuwhit.github.io/tags/AWS/"}]},{"title":"S3에 있는 csv 파일을 AuroraDB로 import 시키기","slug":"import-s3-to-aurora","date":"2017-11-16T14:14:10.000Z","updated":"2017-11-16T15:01:46.000Z","comments":true,"path":"2017/11/16/import-s3-to-aurora/","link":"","permalink":"https://tuwhit.github.io/2017/11/16/import-s3-to-aurora/","excerpt":"","text":"데이터 분석 관련 프로젝트를 진행중이라 간만에 포스팅을 한다. 아마 당분간 프로젝트를 진행하며 겪었던 삽질기를 계속 포스팅하지 않을까 싶음… 처음 해보는것들 투성이라 팀원들이 모두 프로야근러가 되었다. 흑흑ㅠㅠ 오늘은 문서가 잘 정리돼있어서 꽤 쉽게 클리어했던 S3에 있는 csv 파일을 AuroraDB에 import 시키는 방법을 정리해본다. AuroraDB instance를 생성하고 IAM role(case는 물론 RDS로 선택)을 만들어준다. 생성후 permission 탭에서 Attach policy를 클릭하여 S3에 Access할수 있도록 AmazonS3FullAccess 를 추가해준다. 그리고 다시 RDS로 돌아가 Parameter group을 생성해준다. 여기서 Type은 DB Cluster Parameter Group 으로 설정한다. 생성후에 aurora_load_from_s3_role 의 value 값을 위에서 만든 IAM role ARN로 입력해준다. cluster 메뉴에서 해당 cluster 수정 페이지로 이동하여 DB Cluster Parameter Group을 위에서 생성해준 걸로 설정해준다. 다음 페이지로 넘어가서 Apply Immediately 선택후 완료. 다시 cluster 메뉴로 돌아와 해당 cluster 선택 후 Manage IAM roles 페이지로 이동하여 위에서 만들어준 IAM role을 추가해준다. 여기까지가 데이터를 로드하기위한 준비 끝. 1mysql -h [DB Endpoint] -P [port] -u [user name] -p 이후 패스워드를 입력하면 해당 AuroraDB로 접속하게된다. 123load data from s3 &apos;s3-[region]://[bucket name]/[file name]&apos;into table [table name]fields terminated by &apos;,&apos;; 이렇게 하면 s3에 있는 csv 파일의 데이터를 지정한 테이블로 로드하게 된다. 1억 row 짜리 데이터 올리는데 한시간가량 걸렸던것 같음. 위 명령어만으로 로드하면 필드명으로 구분이 안되고 파일에 입력된 순서대로 DB에 저장되니 유의하자. 로드할때 여러가지 옵션이 있는데 여기에서 확인할 수 있다.","categories":[],"tags":[{"name":"S3","slug":"S3","permalink":"https://tuwhit.github.io/tags/S3/"},{"name":"AWS","slug":"AWS","permalink":"https://tuwhit.github.io/tags/AWS/"},{"name":"AuroraDB","slug":"AuroraDB","permalink":"https://tuwhit.github.io/tags/AuroraDB/"},{"name":"Database","slug":"Database","permalink":"https://tuwhit.github.io/tags/Database/"},{"name":"RDS","slug":"RDS","permalink":"https://tuwhit.github.io/tags/RDS/"}]},{"title":"AWS 도커 컨테이너 배포 자동화 실습","slug":"code-deploy","date":"2017-10-27T12:51:05.000Z","updated":"2017-11-16T14:12:41.000Z","comments":true,"path":"2017/10/27/code-deploy/","link":"","permalink":"https://tuwhit.github.io/2017/10/27/code-deploy/","excerpt":"","text":"3일전 Gaming on AWS에 참석해서 ‘도커 컨테이너 배포 자동화 실습’을 진행했다. 회사에서도 도커 컨테이너를 이용해서 배포를 하고있어서(수동이지만) 적용해볼 수 있을것 같다. 당일에는 실습자료를 보고 따라하기 급급했어서 다시 한번 정리해봤다. CodePipeline으로 배포 프로세스를 구성하고 S3에 저장된 소스코드를 CodeBuild를 통해 컴파일 및 컨테이너 이미지 생성한다. 그리고 CloudFormation을 이용해 ECS에 배포한다. CodePipeline 생성 Source provider는 소스코드가 저장된 S3 버킷으로 설정하고 CodeBuild를 build provider로 선택 코드가 build되어 생성된 컨테이너 이미지는 ECR repository에 저장 환경 설정 파일도 S3에 저장하고 CodePipeline 에서 해당 파일을 사용하는 Source 액션 추가 Dockerize 하는 액션 추가 (여기까지가 Build 스테이지) Deploy 스테이지 추가 CloudFormation으로 환경설정 파일을 참조하여 deploy 설정 완료 후 S3에 환경 설정 파일, 어플리케이션을 업로드하면 CodePipeline 에서 변경사항을 감지하여 배포작업을 수행한다. 왕신기. 추가적으로 Lambda를 이용해서 Blue/Green 배포도 가능하다. 현재 리얼 환경이 아닌 곳으로 배포하고 수동 승인과정을 거치면 환경을 Swap 하도록 할 수 있다. 이거슨 빙산의 일각이다 간단히 정리해놔서 그렇지 실제로 콘솔에서 설정해줘야 하는 것들이 굉장히 많았다. (ECS, ELB, ECR, IAM 등등…) 그 하나하나를 이해하기는 좀 어려워서 따로 공부가 필요할 것 같다. 그리고 실습에서 제공된 환경설정파일들도 실제 서비스 환경설정파일들이랑 같이 보면 좋을듯.","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://tuwhit.github.io/tags/AWS/"},{"name":"deploy","slug":"deploy","permalink":"https://tuwhit.github.io/tags/deploy/"},{"name":"CodePipeline","slug":"CodePipeline","permalink":"https://tuwhit.github.io/tags/CodePipeline/"}]},{"title":"개복치같은 서버를 살리기위한 삽질기(ing)","slug":"server-sabjil","date":"2017-10-13T11:15:21.000Z","updated":"2017-10-13T11:29:18.000Z","comments":true,"path":"2017/10/13/server-sabjil/","link":"","permalink":"https://tuwhit.github.io/2017/10/13/server-sabjil/","excerpt":"","text":"작년에 신규 프로젝트를 진행하면서 기존 ruby 서버가 아닌 python 으로 별도의 서버를 띄웠다. Java로 된 별도의 암호화모듈을 사용해야해서 Py4J을 이용했다. 그런데 이 라이브러리의 문제인지 메모리 누수 때문인지 원인은 모르겠지만 2-3주정도 주기로 암호화모듈쪽에서 에러가 나기 시작하면서 그 이후의 모든 요청에서 같은 에러가 발생했다. 기존에 url lib 를 썼던것을 requests 를 사용하도록 수정했지만 에러 발생주기가 조금 길어졌을 뿐 해결이 되지않았다. 일단 원인을 찾는것은 뒤로 미루고 Lambda와 Cloud Watch를 이용해서 매일 새벽2시에 인스턴스를 리붓시키도록 했다. 1234567891011var AWS = require('aws-sdk');exports.handler = (event, context, callback) =&gt; &#123; var ec2 = new AWS.EC2(&#123;region: 'ap-northeast-2'&#125;); ec2.rebootInstances(&#123;InstanceIds : ['instance-ID'] &#125;,function (err, data) &#123; if (err) console.log(err.stack); else console.log(data); context.done(err,data); &#125;);&#125;; 이후 몇달간 서버에 이상이 없는듯 하더니 어느날 아예 500에러가 나기시작했다. -_- 원인을 찾아보려 서버에 접속해봤더니 컨테이너가 아예 없었다. 리붓하면서 컨테이너가 제대로 생성되지 않은것 같았다. Docker 관련 지식이 얕아 원인을 알수가 없어서 일단 인스턴스를 새로 생성하는 것으로 해결을 했다. 그리고 UptimeRobot을 이용해 5분에 한번 서버에 request를 보내고, 문제가 있으면 slack으로 alert message를 남기도록 했다. 이걸 5분만에 설정할수있다니 21세기 스고이 그런데 깜빡하고 Lambda 코드에 바뀐 인스턴스 ID로 수정하지 않아서(아오) 3주뒤 또 암호화 모듈 에러가 발생했다. 이런 휴먼에러를 발생시키지 않기위해 코드를 몇줄 더 추가했다. ㅠㅠ12345678910111213141516171819202122232425262728293031323334353637383940414243444546var AWS = require('aws-sdk');var https = require('https');var util = require('util');var POST_OPTIONS = &#123; hostname: 'hooks.slack.com', path: 'slack web-hook url', method: 'POST',&#125;;exports.handler = (event, context, callback) =&gt; &#123; var ec2 = new AWS.EC2(&#123;region: 'ap-northeast-2'&#125;); const failed_message = &#123; channel: 'service-alerts', text: 'Instace Reboot Failed' &#125;; // check if instance exists ec2.describeInstances(&#123;InstanceIds : ['instance-ID']&#125;, function (err, data)&#123; if (err) &#123; // if doesn't exist, send slack alert var r = https.request(POST_OPTIONS, function(res) &#123; res.setEncoding('utf8'); res.on('data', function (data) &#123; context.succeed(\"Message Sent: \" + data); &#125;); &#125;).on(\"error\", function(e) &#123;context.fail(\"Failed: \" + e);&#125; ); r.write(util.format(\"%j\", failed_message)); r.end(); &#125; else &#123; // exist // reboot instance ec2.rebootInstances(&#123;InstanceIds : ['instance-ID'] &#125;,function (err, data) &#123; if (err) &#123; // send slack alert var r = https.request(POST_OPTIONS, function(res) &#123; res.setEncoding('utf8'); res.on('data', function (data) &#123; context.succeed(\"Message Sent: \" + data); &#125;); &#125;).on(\"error\", function(e) &#123;context.fail(\"Failed: \" + e);&#125; ); r.write(util.format(\"%j\", failed_message)); r.end(); &#125; else console.log(data); context.done(err,data); &#125;); &#125; &#125;);&#125;; 리붓시킬 인스턴스를 먼저 찾고 없으면 슬랙으로 alert message를 보내고, 있으면 해당 인스턴스를 리붓시킨다. 리붓시 에러가 나도 슬랙으로 alert message를 보낸다. 애초에 에러의 원인을 찾아서 해결했어야하는데 다른일들로 시간적 여유가 없다보니 그때그때 서버만 살리고 뒷전으로 미뤄뒀던 것에 반성을… 틈틈히 시간내서 원인파악을 해야겠다.","categories":[],"tags":[{"name":"Server","slug":"Server","permalink":"https://tuwhit.github.io/tags/Server/"},{"name":"Lambda","slug":"Lambda","permalink":"https://tuwhit.github.io/tags/Lambda/"},{"name":"node.js","slug":"node-js","permalink":"https://tuwhit.github.io/tags/node-js/"},{"name":"UptimeRobot","slug":"UptimeRobot","permalink":"https://tuwhit.github.io/tags/UptimeRobot/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://tuwhit.github.io/tags/Monitoring/"}]},{"title":"UI와 UX에 대한 개념 정리","slug":"til","date":"2017-09-26T13:48:04.000Z","updated":"2017-09-26T15:03:21.000Z","comments":true,"path":"2017/09/26/til/","link":"","permalink":"https://tuwhit.github.io/2017/09/26/til/","excerpt":"","text":"프로젝트 진행중에 UX 설계를 해볼일이 생겼는데, 디알못이라 뭘 어떻게 해야할지 영 감이 잘 안잡혀서 일단 UI, UX의 개념이라도 정리해봤다.(정리하다보니 생각났는데 옛날에 Human Centered Design 스터디를 한적이 있는데, 그것도 UX Design에 속하는 거였나 싶음) UI(User Interface) 일련의 화면, 페이지 및 장치와 상호작용할 때 사용하는 시각적 요소 UX(User Experience) 회사, 서비스 및 제품과 상호 작용하는 사용자의 경험 UX Design 실제 필드에서 사용자를 관찰하고, 그 결과를 바탕으로 디자인 누가, 왜, 무엇을, 어떻게 사용하는지를 고려해야함 어떤 목적인지, 어떤 의도가 있는지, 어떻게 쓰였으면 좋겠는지에 대해 고민 필요 UI와 UX의 차이점(에 대한 전문가들의 생각) UX는 문제를 해결하기위한 사용자의 여정에 포커스를 두고 UI는 제품 표면의 모습과 기능에 초점을 맞춘다. - Ken Norton UX 디자이너는 디자인 프로세스의 개념적 측면에 관심을 가지기 때문에 UI 디자이너는 보다 확실한 요소에 집중할 수 있다. - Andy Budd UX와 UI 디자인은 서로 비교할 수 없는 두가지이므로 차이가 없다. -Craig Morrision UI는 일반적으로 화면 주변의 시각적 디자인 및 정보 디자인에 관한것이다. UX는 완벽한 경험에 관한 것이므로 화면과 관련이 없을 수 있다. - Patrick Neema UI는 제품에 중점을 두고 있으며 일련의 스냅샷을 제시간에 제공한다. UX는 사용자 및 제품을 통한 이동에 초점을 맞춘다. - Scott Jenson UX는 사용자가 제품을 사용하여 얻은 전반적인 경험이며 UI는 사용자가 실제로 상호 작용하고 볼수있는 것이다. - Clayton Yan API 로직을 고려하면서 UX 설계를 하려니 쉬운일이 아닌것같다. UX를 우선으로 하자니 로직 개선하기가 빡세고, 쉽게 가자니 UX가 좋지 않은것 같고… 엉엉","categories":[],"tags":[{"name":"UI","slug":"UI","permalink":"https://tuwhit.github.io/tags/UI/"},{"name":"UX","slug":"UX","permalink":"https://tuwhit.github.io/tags/UX/"},{"name":"Design","slug":"Design","permalink":"https://tuwhit.github.io/tags/Design/"}]},{"title":"블로그 개설","slug":"first-post","date":"2017-09-24T12:00:45.000Z","updated":"2017-09-26T12:21:32.000Z","comments":true,"path":"2017/09/24/first-post/","link":"","permalink":"https://tuwhit.github.io/2017/09/24/first-post/","excerpt":"","text":"Github Pages, Hexo 로 블로그 개설! Jekyll을 이용해서 만들까 하다가 Ruby는 회사에서 많이 쓰니 Node.js 기반인 Hexo로 선택했다. 근데 뭐 Node.js 까막눈이어도 눈칫껏 만져보니 기존 테마에 살짝 커스텀하는 정도는 금방 하는듯. Markdown 문법도 잘 몰라서 Markdown 작성법 보면서 이래저래 써보는 중인데 꽤 재밌다. 블로그 메뉴를 어떻게 나눌지, footer 엔 어떤 위젯을 넣을지, 로고는 뭘로 할지 고민. 10대때 열심히 네이버 블로그 꾸미던 시절 생각난다. 하악하악 재밌어! Daily 테마 위키에 comment field 추가하려면 _config.yml 파일에disqus_shortname: your-disqus-shortname 만 추가해주면 된다고 했는데 코멘트 영역이 안뜸…^ㅅㅠ 다행히 disqus 홈페이지에 들어가보니 site - installation 메뉴에 코드가 제공돼있었다.1234567891011121314151617181920&lt;div id=\"disqus_thread\"&gt;&lt;/div&gt;&lt;script&gt;/*** RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*//*var disqus_config = function () &#123;this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variablethis.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable&#125;;*/(function() &#123; // DON'T EDIT BELOW THIS LINEvar d = document, s = d.createElement('script');s.src = 'https://tuwhit.disqus.com/embed.js';s.setAttribute('data-timestamp', +new Date());(d.head || d.body).appendChild(s);&#125;)();&lt;/script&gt;&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=\"https://disqus.com/?ref_noscript\"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt; Disqus가 들어갈 부분에 위 코드를 넣으니 해결! 테마 적용후 deploy 했는데 반영되는데 시간이 꽤 걸리나보다. 계속 깨져보이길래 제대로 deploy 안된줄알고 구글링중이었는데, 몇분 지나니까 잘 반영돼 있음.","categories":[],"tags":[{"name":"blog","slug":"blog","permalink":"https://tuwhit.github.io/tags/blog/"},{"name":"github","slug":"github","permalink":"https://tuwhit.github.io/tags/github/"},{"name":"hexo","slug":"hexo","permalink":"https://tuwhit.github.io/tags/hexo/"}]}]}